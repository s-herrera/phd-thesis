{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed5aa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import f1_score\n",
    "import scipy\n",
    "\n",
    "def parse_features(lst):\n",
    "    \"\"\"\n",
    "    Parses a list of feature names into groups based on a \"class=value\" pattern.\n",
    "    Features not matching the pattern are treated as individual groups.\n",
    "    \"\"\"\n",
    "    groups = defaultdict(list)\n",
    "    for i, name in enumerate(lst):\n",
    "        if '=' in name:\n",
    "            class_name = name.split('=', 1)[0]\n",
    "            if \"rel\" in class_name:\n",
    "                class_name = name.split(\"_\",1)[0]\n",
    "            groups[class_name].append(i)\n",
    "        else:\n",
    "            groups[name].append(i)\n",
    "    return dict(groups)\n",
    "\n",
    "def grouped_permutation_importance(\n",
    "    model,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    feature_names,\n",
    "    scoring_func=f1_score,\n",
    "    n_repeats=10,\n",
    "    random_state=None\n",
    "):\n",
    "\n",
    "    feature_groups = parse_features(feature_names)\n",
    "    group_names = list(feature_groups.keys())\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    is_sparse_matrix = scipy.sparse.issparse(X_test)\n",
    "    # baseline score. Do I have to compute it several times?\n",
    "    y_pred_base = model.predict(X_test)\n",
    "    baseline_score = scoring_func(y_test, y_pred_base, average=\"macro\")\n",
    "    raw_importances = defaultdict(list)\n",
    "    for group_name in group_names:\n",
    "        group_indices = feature_groups[group_name]\n",
    "        if not group_indices:\n",
    "            raw_importances[group_name] = [0.0] * n_repeats # Importance is 0 if no columns\n",
    "            continue\n",
    "\n",
    "        for _ in range(n_repeats):\n",
    "            X_permuted = X_test.copy()\n",
    "            if is_sparse_matrix:\n",
    "                X_permuted = X_permuted.tolil() # to lil because is faster\n",
    "\n",
    "            for col_idx in group_indices:\n",
    "                col = X_permuted[:, col_idx].toarray().ravel()\n",
    "                permuted_col = rng.permutation(col)\n",
    "                X_permuted[:, col_idx] = np.reshape(permuted_col, (-1, 1))\n",
    "            \n",
    "            X_permuted = X_permuted.tocsr()\n",
    "            y_pred_permuted = model.predict(X_permuted)\n",
    "            permuted_score = scoring_func(y_test, y_pred_permuted, average=\"macro\")\n",
    "            raw_importances[group_name].append(baseline_score - permuted_score)\n",
    "            \n",
    "    importances_mean = np.array([np.mean(raw_importances[name]) for name in group_names])\n",
    "    importances_std = np.array([np.std(raw_importances[name]) for name in group_names])\n",
    "    \n",
    "    sorted_indices = np.argsort(importances_mean)[::-1]\n",
    "    sorted_group_names = [group_names[i] for i in sorted_indices]\n",
    "    sorted_importances_mean = importances_mean[sorted_indices]\n",
    "    sorted_importances_std = importances_std[sorted_indices]\n",
    "    res = {\n",
    "        \"importances_mean\": sorted_importances_mean,\n",
    "        \"importances_std\": sorted_importances_std,\n",
    "        \"group_names\": sorted_group_names,\n",
    "        \"baseline_score\": baseline_score\n",
    "    }\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0271de",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf181130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import build_occurrences, matrix\n",
    "\n",
    "path = \"data/SUD_French-GSD-r2.15/preprocessed_data\"\n",
    "patterns = \"patterns/patterns_subject_inv_with_gov_lemmas.txt\"\n",
    "data = build_occurrences(path, patterns, \"sud\")\n",
    "X, y, feature_names = matrix(data, max_degree=1)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d610797d",
   "metadata": {},
   "source": [
    "## Permutation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcbf730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import skglm\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "model = skglm.SparseLogisticRegression(\n",
    "    fit_intercept=True,\n",
    "    max_iter=20,\n",
    "    max_epochs=1000,\n",
    "    alpha=0.001\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "    \n",
    "# grouped permutation importance\n",
    "importance_results = grouped_permutation_importance(\n",
    "    model=model,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test, \n",
    "    feature_names=feature_names, \n",
    "    scoring_func=f1_score, \n",
    "    n_repeats=30,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nBaseline Validation Accuracy: {importance_results['baseline_score']:.4f}\")\n",
    "print(\"\\nGrouped Permutation Importances (Mean Decrease in Macro F1):\")\n",
    "for i in range(len(importance_results['group_names'])):\n",
    "    group = importance_results['group_names'][i]\n",
    "    mean_imp = importance_results['importances_mean'][i]\n",
    "    std_imp = importance_results['importances_std'][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fe21a5",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812743fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "def class_feature_to_request(s):\n",
    "    _, name, node, feature = s.split(\":\")\n",
    "    if feature == \"len\" : feature = \"length\"\n",
    "    if node == \"own\":\n",
    "        class_name = f\"{name}.{feature}\"\n",
    "    else:\n",
    "        class_name = f\"{name}{node}.{feature}\"\n",
    "    return class_name\n",
    "\n",
    "def plot_permutation_importances(\n",
    "    group_names,\n",
    "    importances_mean,\n",
    "    importances_std,\n",
    "    title=\"Grouped Permutation Importances\",\n",
    "    metric_name=\"Decrease in Score\",\n",
    "    reliability_std_factor=2.0,\n",
    "    epsilon_threshold=1e-8\n",
    "):\n",
    "\n",
    "    if not (len(group_names) == len(importances_mean) == len(importances_std)):\n",
    "        raise ValueError(\"Input lists/arrays must have the same length.\")\n",
    "\n",
    "    means_orig = np.array(importances_mean)\n",
    "    stds_orig = np.array(importances_std)\n",
    "    names_orig = np.array([class_feature_to_request(name) for name in group_names])\n",
    "\n",
    "    is_negligible_mask = (means_orig < epsilon_threshold)\n",
    "    \n",
    "    plot_means = means_orig[~is_negligible_mask]\n",
    "    plot_stds = stds_orig[~is_negligible_mask]\n",
    "    plot_names = names_orig[~is_negligible_mask]\n",
    "\n",
    "    if len(plot_names) == 0:\n",
    "        print(f\"No features to plot after filtering those with mean and std < {epsilon_threshold:.1e}.\")\n",
    "        return\n",
    "\n",
    "    sorted_indices = np.argsort(plot_means)\n",
    "    \n",
    "    plot_names_sorted = plot_names[sorted_indices]\n",
    "    plot_means_sorted = plot_means[sorted_indices]\n",
    "    plot_stds_sorted = plot_stds[sorted_indices]\n",
    "\n",
    "    # 4 categories of importance given the standard deviation on importance\n",
    "    reliably_positive = \"reliably_positive\"\n",
    "    uncertain_positive = \"uncertain_positive\"\n",
    "    reliably_negative = \"reliably_negative\"\n",
    "    uncertain_negative = \"uncertain_negative\"\n",
    "    other = \"other\"\n",
    "\n",
    "    legend_labels_map = {\n",
    "        reliably_positive: f\"Mean - {reliability_std_factor:.1f}*SD > {epsilon_threshold:.0e}\",\n",
    "        uncertain_positive: f\"Mean > {epsilon_threshold:.0e}\",\n",
    "        reliably_negative: f\"Mean + {reliability_std_factor:.1f}*SD < -{epsilon_threshold:.0e}\",\n",
    "        uncertain_negative: f\"Mean < -{epsilon_threshold:.0e}\",\n",
    "        other: f\"Near Zero\"\n",
    "    }\n",
    "\n",
    "    palette = sns.color_palette(\"Set2\", n_colors=5) \n",
    "    category_color_map = {\n",
    "        reliably_positive: palette[0],\n",
    "        uncertain_positive: palette[1],\n",
    "        reliably_negative: palette[2],\n",
    "        uncertain_negative: palette[3],\n",
    "        other: palette[4]\n",
    "    }\n",
    "    \n",
    "    colors_for_plot = []\n",
    "    actual_legend_labels_in_plot = [] # the full legend labels that are used\n",
    "\n",
    "    for mean_val, std_val in zip(plot_means_sorted, plot_stds_sorted):\n",
    "        internal_cat_key = other # default category\n",
    "        \n",
    "        lower_bound_positive = mean_val - reliability_std_factor * std_val\n",
    "        upper_bound_negative = mean_val + reliability_std_factor * std_val\n",
    "\n",
    "        if lower_bound_positive > epsilon_threshold:\n",
    "            internal_cat_key = reliably_positive\n",
    "        elif upper_bound_negative < -epsilon_threshold:\n",
    "            internal_cat_key = reliably_negative\n",
    "        elif mean_val > epsilon_threshold:\n",
    "            internal_cat_key = uncertain_positive\n",
    "        elif mean_val < -epsilon_threshold: \n",
    "            internal_cat_key = uncertain_negative\n",
    "\n",
    "        colors_for_plot.append(category_color_map[internal_cat_key])\n",
    "        actual_legend_labels_in_plot.append(legend_labels_map[internal_cat_key])\n",
    "\n",
    "    # plotting\n",
    "    y_pos = np.arange(len(plot_names_sorted))\n",
    "    fig, ax = plt.subplots(figsize=(12, max(6, len(plot_names_sorted) * 0.45)))\n",
    "    \n",
    "    ax.barh(y_pos, plot_means_sorted, xerr=plot_stds_sorted, align='center',\n",
    "            color=colors_for_plot, capsize=4, ecolor='dimgray')\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(plot_names_sorted)\n",
    "    ax.set_xlabel(f\"Mean Importance ({metric_name})\")\n",
    "    ax.set_title(title)\n",
    "    ax.axvline(0, color='black', linewidth=0.8, linestyle='--')\n",
    "    \n",
    "    # 5. Create custom legend\n",
    "    legend_handles = []\n",
    "    seen_legend_labels = set()\n",
    "    \n",
    "    # order for legend entries to appear consistently\n",
    "    preferred_legend_order_labels = [\n",
    "        legend_labels_map[reliably_positive],\n",
    "        legend_labels_map[uncertain_positive],\n",
    "        legend_labels_map[uncertain_negative],\n",
    "        legend_labels_map[reliably_negative],\n",
    "        legend_labels_map[other]\n",
    "    ]\n",
    "\n",
    "    for label_text in preferred_legend_order_labels:\n",
    "        if label_text in actual_legend_labels_in_plot and label_text not in seen_legend_labels:\n",
    "            internal_key_for_label = None\n",
    "            for key, val_label in legend_labels_map.items():\n",
    "                if val_label == label_text:\n",
    "                    internal_key_for_label = key\n",
    "                    break\n",
    "            if internal_key_for_label:\n",
    "                 legend_handles.append(Patch(facecolor=category_color_map[internal_key_for_label], label=label_text))\n",
    "                 seen_legend_labels.add(label_text)\n",
    "            \n",
    "    if legend_handles:\n",
    "        ax.legend(handles=legend_handles, title=\"Importance Interpretation\", \n",
    "                  bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0.)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 0.72, 1] if legend_handles else None)\n",
    "    plt.savefig(\"permutation_test_subj.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8ce762",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_permutation_importances(\n",
    "    group_names=importance_results['group_names'],\n",
    "    importances_mean=importance_results['importances_mean'],\n",
    "    importances_std=importance_results['importances_std'],\n",
    "    title=\"Feature Group Importances (Permutation Test)\\nScope: X-[1=subj]->Y\\nQ: X << Y\",\n",
    "    metric_name=\"Decrease in Macro F1-score\",\n",
    "    reliability_std_factor=2.0\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
